{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4e1bdf",
   "metadata": {},
   "source": [
    "# üß¨ Lab 3 ‚Äî Visualization and Dimensionality Reduction in Bioinformatics\n",
    "\n",
    "This lab explores **dimensionality reduction** and **clustering** for biological datasets such as RNA-seq.\n",
    "We use both classical linear methods (PCA) and nonlinear methods (t-SNE, UMAP, Autoencoders).\n",
    "\n",
    "### Covered Topics\n",
    "- PCA (Principal Component Analysis)\n",
    "- t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "- UMAP (Uniform Manifold Approximation and Projection)\n",
    "- k-Means and Hierarchical Clustering\n",
    "- PyTorch Autoencoder for nonlinear dimensionality reduction\n",
    "\n",
    "‚öôÔ∏è *Some heavy computations (t-SNE, UMAP) are mocked here but full runnable code is provided (commented out) for students with GPU or internet access.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945ee84",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For autoencoders, preferably on Linux/Mac, uncomment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd131c",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option A: Load a real GEO dataset (requires internet)\n",
    "try:\n",
    "    import GEOparse\n",
    "    print(\"Downloading GSE10072 from GEO...\")\n",
    "    gse = GEOparse.get_GEO(\"GSE10072\", destdir=\"./\")\n",
    "    \n",
    "    # Get expression data\n",
    "    expr_data = gse.pivot_samples('VALUE')\n",
    "    \n",
    "    # Get sample metadata\n",
    "    sample_metadata = gse.phenotype_data\n",
    "    \n",
    "    # Create conditions based on sample characteristics\n",
    "    # For GSE37704, we'll use a simple condition assignment\n",
    "    n_samples = expr_data.shape[1]\n",
    "    conditions = np.repeat([\"Control\", \"Treatment\"], n_samples // 2)\n",
    "    if n_samples % 2 == 1:  # Handle odd number of samples\n",
    "        conditions = np.append(conditions, [\"Control\"])\n",
    "    \n",
    "    # Create gene names\n",
    "    n_genes = expr_data.shape[0]\n",
    "    gene_names = [f\"Gene_{i}\" for i in range(n_genes)]\n",
    "    \n",
    "    # Create data DataFrame\n",
    "    data = pd.DataFrame(expr_data.values, \n",
    "                       index=gene_names, \n",
    "                       columns=[f\"Sample_{i}\" for i in range(n_samples)])\n",
    "    \n",
    "    # Create metadata DataFrame\n",
    "    meta = pd.DataFrame({\n",
    "        \"Sample\": data.columns, \n",
    "        \"Condition\": conditions\n",
    "    })\n",
    "    \n",
    "    print(f\"Successfully loaded GEO dataset: {n_genes} genes, {n_samples} samples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to download GEO data: {e}\")\n",
    "    print(\"Using synthetic dataset instead...\")\n",
    "    \n",
    "    # Option B: Synthetic dataset fallback\n",
    "    np.random.seed(42)\n",
    "    n_genes, n_samples = 500, 60\n",
    "    conditions = np.repeat([\"Control\", \"Treatment\"], n_samples // 2)\n",
    "    \n",
    "    # Generate synthetic expression data\n",
    "    expr_data = np.random.negative_binomial(n=20, p=0.5, size=(n_genes, n_samples)).astype(float)\n",
    "    # Add treatment effect\n",
    "    expr_data[:, conditions == \"Treatment\"] += np.random.normal(10, 5, (n_genes, sum(conditions == \"Treatment\")))\n",
    "    \n",
    "    # Create data and metadata\n",
    "    data = pd.DataFrame(expr_data, \n",
    "                       index=[f\"Gene_{i}\" for i in range(n_genes)], \n",
    "                       columns=[f\"Sample_{i}\" for i in range(n_samples)])\n",
    "    meta = pd.DataFrame({\"Sample\": data.columns, \"Condition\": conditions})\n",
    "    \n",
    "    print(f\"Using synthetic dataset: {n_genes} genes, {n_samples} samples\")\n",
    "\n",
    "# Log transform and scaling\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Check for and handle NaN values\n",
    "data_log = np.log1p(data)\n",
    "print(f\"NaN values in log-transformed data: {np.isnan(data_log).sum()}\")\n",
    "\n",
    "# Remove NaN values by replacing with 0 (or you could use median/mean)\n",
    "data_log_clean = data_log.fillna(0) if hasattr(data_log, 'fillna') else np.nan_to_num(data_log, nan=0.0)\n",
    "\n",
    "# Transpose and scale\n",
    "X_scaled = StandardScaler().fit_transform(data_log_clean.T)\n",
    "\n",
    "# Final check for NaN values\n",
    "print(f\"NaN values in scaled data: {np.isnan(X_scaled).sum()}\")\n",
    "print(f\"Data shape after preprocessing: {X_scaled.shape}\")\n",
    "print(f\"Conditions: {meta['Condition'].value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb73c2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beefb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA of the dataset in 2 and 3 dimensions\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Make a PCA decomposition to 2 and 3 dimensions and show it as a plot\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_3 = PCA(n_components=3)\n",
    "\n",
    "X_pca_2 = pca_2.fit_transform(X_scaled)\n",
    "X_pca_3 = pca_3.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.scatter(X_pca_2[:, 0], X_pca_2[:, 1], c=meta[\"Condition\"].map({\"Control\": 'red', \"Treatment\": 'blue'}))\n",
    "ax1.set_xlabel(f\"Explained Variance: {pca_2.explained_variance_ratio_[0]*100:.1f}%\")\n",
    "ax1.set_ylabel(f\"Explained Variance: {pca_2.explained_variance_ratio_[1]*100:.1f}%\")\n",
    "ax1.set_title(\"2-component PCA\")\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection=\"3d\")\n",
    "ax2.scatter(X_pca_3[:, 0], X_pca_3[:, 1], X_pca_3[:, 2], c=meta[\"Condition\"].map({\"Control\": 'red', \"Treatment\": 'blue'}))\n",
    "ax2.set_xlabel(f\"Explained Variance: {pca_3.explained_variance_ratio_[0]*100:.1f}%\")\n",
    "ax2.set_ylabel(f\"Explained Variance: {pca_3.explained_variance_ratio_[1]*100:.1f}%\")\n",
    "ax2.set_zlabel(f\"Explained Variance: {pca_3.explained_variance_ratio_[2]*100:.1f}%\")\n",
    "ax2.set_title(\"3-component PCA\")\n",
    "ax2.set_box_aspect(None, zoom=0.75)\n",
    "# TODO: make a PCA decomposition to 2 and 3 dimensions and show it as a plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c39347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative explained variance to determine optimal number of components\n",
    "# What do you think about the data?\n",
    "cev = []\n",
    "for i in range(1, 51):\n",
    "    pca = PCA(i)\n",
    "    X_fit = pca.fit_transform(X_scaled)\n",
    "    cev.append(sum(pca.explained_variance_ratio_)*100)\n",
    "\n",
    "plt.plot(cev, marker=\"o\", markersize=4, linestyle=\"solid\")\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance [%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85688cfb",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ t-SNE ‚Äî t-distributed Stochastic Neighbor Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform t-SNE dimensionality reduction in 2D and 3D\n",
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# perform t-SNE decomposition of X_scaled to 2 and 3 dimensions. \n",
    "# Calculate the number of iterations needed for convergence and KL-divergence\n",
    "tsne_2 = TSNE(n_components=2)\n",
    "tsne_3 = TSNE(n_components=3)\n",
    "\n",
    "X_tsne_2 = tsne_2.fit_transform(X_scaled)\n",
    "X_tsne_3 = tsne_3.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.scatter(X_tsne_2[:, 0], X_tsne_2[:, 1], c=meta[\"Condition\"].map({\"Control\": 'red', \"Treatment\": 'blue'}))\n",
    "ax1.set_title(\"2-component T-SNE\")\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection=\"3d\")\n",
    "ax2.scatter(X_tsne_3[:, 0], X_tsne_3[:, 1], X_tsne_3[:, 2], c=meta[\"Condition\"].map({\"Control\": 'red', \"Treatment\": 'blue'}))\n",
    "ax2.set_title(\"3-component T-SNE\")\n",
    "ax2.set_box_aspect(None, zoom=0.75)\n",
    "\n",
    "print(f\"2-component T-SNE:\")\n",
    "print(f\"no. of iterations: {tsne_2.n_iter_}\")\n",
    "print(f\"KL-divergence: {tsne_2.kl_divergence_:0.3f}\\n\")\n",
    "\n",
    "print(f\"3-component T-SNE:\")\n",
    "print(f\"no. of iterations: {tsne_3.n_iter_}\")\n",
    "print(f\"KL-divergence: {tsne_3.kl_divergence_:0.3f}\")\n",
    "# TODO perform t-SNE decomposition of X_scaled to 2 and 3 dimensions. \n",
    "# Calculate the number of iterations needed for convergence and KL-divergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a2b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze t-SNE Stability Analysis: Different Random Seeds \n",
    "# Plot a 1x4 figure differing in seeds\n",
    "print(\"Computing t-SNE with different random seeds to assess stability...\")\n",
    "\n",
    "# Use the same perplexity as before, different seeds\n",
    "perplexity = min(30, (X_scaled.shape[0] - 1) // 3)\n",
    "seeds = [42, 123, 456, 789]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(1, 5):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    ax = fig.add_subplot(2, 2, i)\n",
    "    ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=meta[\"Condition\"].map({\"Control\": 'red', \"Treatment\": 'blue'}))\n",
    "    ax.set_title(\"2-component T-SNE\")\n",
    "seeds = [42, 123, 456, 789]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: t-SNE Parameter Analysis: Different Perplexity Values\n",
    "# Do the same, but changing perplexity\n",
    "\n",
    "print(\"Computing t-SNE with different perplexities...\")\n",
    "\n",
    "perplexities = [2, 5, 10, 20]\n",
    "seeds = [42, 123, 456, 789]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(1, 5):\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexities[i-1])\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    ax = fig.add_subplot(2, 2, i)\n",
    "    ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=meta[\"Condition\"].map({\"Control\": 'red', \"Treatment\": 'blue'}))\n",
    "    ax.set_title(f\"2-component T-SNE with perplexity={perplexities[i-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdfaec3",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ UMAP ‚Äî Uniform Manifold Approximation and Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform UMAP dimensionality reduction in 2D and 3D\n",
    "import umap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Perform UMAP decompositino to 2 and 3 dimensions\n",
    "\n",
    "print(\"Computing UMAP embeddings...\")\n",
    "# UMAP is generally faster than t-SNE and preserves both local and global structure...\n",
    "\n",
    "# TODO: Perform UMAP decompositino to 2 and 3 dimensions\n",
    "\n",
    "print(\"Computing UMAP embeddings...\")\n",
    "print(\"UMAP is generally faster than t-SNE and preserves both local and global structure...\")\n",
    "\n",
    "# Calculate UMAP with 3 components to get both 2D and 3D projections\n",
    "# UMAP parameters: n_neighbors (local neighborhood size) and min_dist (minimum distance between points)\n",
    "n_neighbors = min(15, X_scaled.shape[0] - 1)  # Ensure n_neighbors doesn't exceed sample size\n",
    "min_dist = 0.1  # Minimum distance between points in the embedding\n",
    "\n",
    "umap_2 = umap.UMAP(n_neighbors=n_neighbors, n_components=2, min_dist=min_dist)\n",
    "umap_3 = umap.UMAP(n_neighbors=n_neighbors, n_components=3, min_dist=min_dist)\n",
    "\n",
    "X_umap_2 = umap_2.fit_transform(X_scaled)\n",
    "X_umap_3 = umap_3.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.scatter(X_umap_2[:, 0], X_umap_2[:, 1], c=meta[\"Condition\"].map({\"Control\": 'red', \"Treatment\": 'blue'}))\n",
    "ax1.set_title(\"2-component UMAP\")\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection=\"3d\")\n",
    "ax2.scatter(X_umap_3[:, 0], X_umap_3[:, 1], X_umap_3[:, 2], c=meta[\"Condition\"].map({\"Control\": 'red', \"Treatment\": 'blue'}))\n",
    "ax2.set_title(\"3-component UMAP\")\n",
    "ax2.set_box_aspect(None, zoom=0.75)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5645a9e",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Clustering ‚Äî k-Means and Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887dbfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K-Means clustering on UMAP projection with different numbers of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Perform clustering of UMAP result to different cluster numbers. \n",
    "# Show the results as 2x3 grid\n",
    "# Calculate Silhouette scores for each clustering\n",
    "# TODO perform clustering of UMAP result to different cluster numbers. \n",
    "# Show the results as 2x3 grid\n",
    "# calculate Silhouette scores for each clustering\n",
    "print(\"Performing k-means clustering on UMAP projection...\")\n",
    "\n",
    "# Define different numbers of clusters to test\n",
    "n_clusters_list = [2, 3, 4, 5, 8, 10]\n",
    "silhouette_scores = []\n",
    "\n",
    "# Create 2x3 subplot layout\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "for i in range(6):\n",
    "    kmeans = KMeans(n_clusters=n_clusters_list[i])\n",
    "    X_umap_2_clusters = kmeans.fit_predict(X_umap_2)\n",
    "    silhouette_scores.append(silhouette_score(X_umap_2, X_umap_2_clusters))\n",
    "    ax = fig.add_subplot(2, 3, i + 1)\n",
    "    ax.scatter(X_umap_2[:, 0], X_umap_2[:, 1], c=X_umap_2_clusters)\n",
    "    ax.set_title(f\"K-means with {n_clusters_list[i]} clusters, silh. score: {silhouette_scores[i]:.3f}\")\n",
    "plt.figure(figsize=(18, 12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd279246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score Analysis: Plot silhouette score against number of clusters\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for i in range(2, 51):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    X_umap_2_clusters = kmeans.fit_predict(X_umap_2)\n",
    "    silhouette_scores.append(silhouette_score(X_umap_2, X_umap_2_clusters))\n",
    "\n",
    "plt.plot(silhouette_scores, marker=\"o\", markersize=4, linestyle=\"solid\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Silhouette score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering: Plot dendrograms of UMAP result with different linkage methods\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "print(\"Computing hierarchical clustering with different linkage methods...\")\n",
    "\n",
    "# Define different linkage methods\n",
    "linkage_methods = ['ward', 'complete', 'average']\n",
    "linkage_names = ['Ward', 'Complete', 'Average']\n",
    "\n",
    "linkages = list(map(lambda x: linkage(X_umap_2, method=x), linkage_methods))\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "for i in range(3):\n",
    "    ax = fig.add_subplot(1, 3, i + 1)\n",
    "    dendrogram(linkages[i], ax=ax)\n",
    "    ax.set_title(f\"Linkage method: {linkage_names[i]}\")\n",
    "linkage_names = ['Ward', 'Complete', 'Average']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2f349",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Autoencoder (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Mock Dataset for Autoencoder Compression\n",
    "print(\"Creating mock dataset for autoencoder dimensionality reduction...\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dataset parameters\n",
    "n_samples = 200\n",
    "n_features = 30\n",
    "\n",
    "# Create base dataset with some structure\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Add distinct patterns to create interesting structure\n",
    "# Group 1: Add positive bias to first half of features\n",
    "X[:n_samples//2, :n_features//2] += 2.0\n",
    "\n",
    "# Group 2: Add negative bias to second half of features  \n",
    "X[n_samples//2:, n_features//2:] -= 1.5\n",
    "\n",
    "# Add some noise\n",
    "X += np.random.normal(0, 0.3, X.shape)\n",
    "\n",
    "# Standardize the data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Create labels for visualization\n",
    "labels = np.repeat([\"Pattern A\", \"Pattern B\"], n_samples // 2)\n",
    "if n_samples % 2 == 1:\n",
    "    labels = np.append(labels, [\"Pattern A\"])\n",
    "\n",
    "print(f\"Mock dataset created:\")\n",
    "print(f\"  Shape: {X.shape}\")\n",
    "print(f\"  Features: {n_features}\")\n",
    "print(f\"  Samples: {n_samples}\")\n",
    "print(f\"  Labels: {np.unique(labels, return_counts=True)}\")\n",
    "\n",
    "# Visualize the dataset structure\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Feature means by group\n",
    "plt.subplot(1, 2, 1)\n",
    "group_a = X[labels == \"Pattern A\"]\n",
    "group_b = X[labels == \"Pattern B\"]\n",
    "plt.plot(np.mean(group_a, axis=0), 'b-', label='Pattern A', linewidth=2)\n",
    "plt.plot(np.mean(group_b, axis=0), 'r-', label='Pattern B', linewidth=2)\n",
    "plt.title(\"Mean Feature Values by Pattern\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Mean Value\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: First two features scatter plot\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = (labels == \"Pattern B\").astype(int)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors, cmap=\"coolwarm\", alpha=0.7, s=50)\n",
    "plt.title(\"First Two Features\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Dataset ready for autoencoder compression!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Autoencoder for nonlinear dimensionality reduction\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(), nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64), nn.ReLU(), nn.Linear(64, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_rec = self.decoder(z)\n",
    "        return x_rec, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple Autoencoder Training with Mock Dataset\n",
    "print(\"Creating mock dataset for autoencoder training...\")\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 200, 30  # Small dataset for fast training\n",
    "X_mock = np.random.randn(n_samples, n_features)\n",
    "# Add some structure to make it more interesting\n",
    "X_mock[:n_samples//2] += 1.5  # Create two distinct groups\n",
    "X_mock = StandardScaler().fit_transform(X_mock)\n",
    "\n",
    "# Create mock conditions\n",
    "conditions_mock = np.repeat([\"Group A\", \"Group B\"], n_samples // 2)\n",
    "\n",
    "# Initialize autoencoder with mock data\n",
    "input_dim = X_mock.shape[1]\n",
    "ae = Autoencoder(input_dim)\n",
    "optimizer = optim.Adam(ae.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "tensor_data = torch.tensor(X_mock, dtype=torch.float32)\n",
    "loader = DataLoader(TensorDataset(tensor_data, tensor_data), batch_size=16, shuffle=True)\n",
    "\n",
    "print(\"Training autoencoder for 20 epochs...\")\n",
    "# Train the autoencoder\n",
    "ae.train()\n",
    "losses = []\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_rec, _ = ae(x)\n",
    "        loss = criterion(x_rec, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    losses.append(epoch_loss / len(loader))\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:2d}, Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Extract latent representations\n",
    "ae.eval()\n",
    "with torch.no_grad():\n",
    "    _, X_latent = ae(tensor_data)\n",
    "    X_latent = X_latent.numpy()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Autoencoder Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Latent space\n",
    "plt.subplot(1, 2, 2)\n",
    "condition_colors_mock = (conditions_mock == \"Group B\").astype(int)\n",
    "plt.scatter(X_latent[:, 0], X_latent[:, 1], c=condition_colors_mock, \n",
    "           cmap=\"coolwarm\", alpha=0.7, s=50)\n",
    "plt.title(\"Autoencoder Latent Space\")\n",
    "plt.xlabel(\"Latent Dimension 1\")\n",
    "plt.ylabel(\"Latent Dimension 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Latent space shape: {X_latent.shape}\")\n",
    "print(\"The autoencoder successfully learned a 2D representation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c5eaf",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "In this lab you learned how to:\n",
    "- Represent and scale gene expression data (RNA-seq)\n",
    "- Apply PCA for variance-based projections\n",
    "- Understand t-SNE and UMAP for nonlinear embeddings\n",
    "- Cluster biological samples (k-Means, hierarchical)\n",
    "- Use (and mock) autoencoders for deep feature compression\n",
    "\n",
    "Next steps:\n",
    "‚û° Integrate these visualizations into multi-omics datasets  \n",
    "‚û° Use the embeddings for **classification and biomarker discovery**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
